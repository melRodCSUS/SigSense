{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10705,"sourceType":"datasetVersion","datasetId":7160},{"sourceId":2243895,"sourceType":"datasetVersion","datasetId":1347338},{"sourceId":9715737,"sourceType":"datasetVersion","datasetId":5943650}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install required libraries\n!pip install --quiet pdf2image\n!apt-get install -y poppler-utils\n!pip install --quiet fpdf\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T06:28:59.658179Z","iopub.execute_input":"2024-10-25T06:28:59.658680Z","iopub.status.idle":"2024-10-25T06:29:27.403253Z","shell.execute_reply.started":"2024-10-25T06:28:59.658638Z","shell.execute_reply":"2024-10-25T06:29:27.401630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Load and preprocess data\ndef load_data():\n    # Load training data\n    train_data = pd.read_csv('/kaggle/input/emnist/emnist-letters-train.csv')\n    X_train = train_data.iloc[:, 1:].values\n    y_train = train_data.iloc[:, 0].values\n    \n    # Load test data\n    test_data = pd.read_csv('/kaggle/input/emnist/emnist-letters-test.csv')\n    X_test = test_data.iloc[:, 1:].values\n    y_test = test_data.iloc[:, 0].values\n    \n    # Reshape images to 28x28x1\n    X_train = X_train.reshape(-1, 28, 28, 1)\n    X_test = X_test.reshape(-1, 28, 28, 1)\n    \n    # Normalize pixel values\n    X_train = X_train.astype('float32') / 255.0\n    X_test = X_test.astype('float32') / 255.0\n    \n    return X_train, y_train, X_test, y_test\n\n# Create the forensic analysis model\ndef create_model():\n    input_shape = (28, 28, 1)\n    \n    # Input layer\n    inputs = Input(shape=input_shape)\n    \n    # First convolutional block - Feature extraction\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(0.25)(x)\n    \n    # Second convolutional block - Stroke analysis\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(0.25)(x)\n    \n    # Third convolutional block - Deep features\n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Dropout(0.25)(x)\n    \n    # Flatten and dense layers\n    x = Flatten()(x)\n    x = Dense(512, activation='relu', name='forensic_dense')(x)\n    x = Dropout(0.5)(x)\n    \n    # Output layer - number of classes (26 for letters)\n    outputs = Dense(26, activation='softmax')(x)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Function to extract forensic features\ndef extract_forensic_features(model, image_batch):\n    \"\"\"Extract intermediate features for forensic analysis\"\"\"\n    feature_model = Model(inputs=model.input, outputs=model.get_layer('forensic_dense').output)\n    features = feature_model.predict(image_batch)\n    return features\n\n# Function to compare two characters\ndef compare_characters(model, char1, char2):\n    \"\"\"Compare two characters and return similarity score\"\"\"\n    # Extract features\n    feat1 = extract_forensic_features(model, char1.reshape(1, 28, 28, 1))\n    feat2 = extract_forensic_features(model, char2.reshape(1, 28, 28, 1))\n    \n    # Compute cosine similarity\n    similarity = np.dot(feat1, feat2.T) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))\n    return similarity[0][0]\n\n# Main training and evaluation function\ndef main():\n    print(\"Loading data...\")\n    X_train, y_train, X_test, y_test = load_data()\n    \n    # Adjust labels to 0-based indexing\n    y_train = y_train - 1\n    y_test = y_test - 1\n    \n    print(\"Creating model...\")\n    model = create_model()\n    \n    # Compile model\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                 loss='sparse_categorical_crossentropy',\n                 metrics=['accuracy'])\n    \n    print(\"Training model...\")\n    # Train with early stopping for efficiency\n    history = model.fit(X_train, y_train,\n                       batch_size=128,\n                       epochs=10,  # Reduced epochs for quick training\n                       validation_split=0.1,\n                       verbose=1)\n    \n    print(\"\\nEvaluating model...\")\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    print(f\"Test accuracy: {test_acc:.4f}\")\n    \n    # Demonstrate forensic comparison\n    print(\"\\nDemonstrating character comparison...\")\n    # Compare two random test characters\n    idx1, idx2 = np.random.randint(0, len(X_test), 2)\n    similarity = compare_characters(model, X_test[idx1], X_test[idx2])\n    print(f\"Similarity score between test characters: {similarity:.4f}\")\n    print(f\"True labels: {chr(65 + y_test[idx1])} and {chr(65 + y_test[idx2])}\")\n    \n    return model, history\n\n# Additional utility functions for forensic analysis\ndef analyze_character_features(model, character):\n    \"\"\"Detailed analysis of a single character\"\"\"\n    features = extract_forensic_features(model, character.reshape(1, 28, 28, 1))\n    \n    # Basic statistical analysis\n    feature_stats = {\n        'mean': np.mean(features),\n        'std': np.std(features),\n        'max': np.max(features),\n        'min': np.min(features)\n    }\n    \n    return feature_stats\n\ndef batch_compare_characters(model, reference_char, comparison_chars):\n    \"\"\"Compare one character against multiple others\"\"\"\n    similarities = []\n    for char in comparison_chars:\n        sim = compare_characters(model, reference_char, char)\n        similarities.append(sim)\n    return np.array(similarities)\n\n# Run the main function\nif __name__ == \"__main__\":\n    print(\"Starting forensic handwriting analysis system...\")\n    model, history = main()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T19:43:21.089274Z","iopub.execute_input":"2024-10-25T19:43:21.089629Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Starting forensic handwriting analysis system...\nLoading data...\nCreating model...\nTraining model...\nEpoch 1/10\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.6015 - loss: 1.3126 - val_accuracy: 0.9238 - val_loss: 0.2349\nEpoch 2/10\n\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.9025 - loss: 0.2903 - val_accuracy: 0.9392 - val_loss: 0.1806\nEpoch 3/10\n\u001b[1m621/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9232 - loss: 0.2273","output_type":"stream"}]},{"cell_type":"code","source":"# This will run the complete training and analysis pipeline\nmodel, history = main()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T07:53:36.883712Z","iopub.execute_input":"2024-10-25T07:53:36.884118Z","iopub.status.idle":"2024-10-25T07:54:58.579628Z","shell.execute_reply.started":"2024-10-25T07:53:36.884081Z","shell.execute_reply":"2024-10-25T07:54:58.578747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_training_history(history):\n    # Plot training & validation accuracy values\n    plt.figure(figsize=(12, 4))\n    \n    # Accuracy plot\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    \n    # Loss plot\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    \n    plt.show()\n\n# Assuming `history` is the output from model.fit\nplot_training_history(history)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:01:57.284546Z","iopub.execute_input":"2024-10-25T08:01:57.284939Z","iopub.status.idle":"2024-10-25T08:01:57.808202Z","shell.execute_reply.started":"2024-10-25T08:01:57.284902Z","shell.execute_reply":"2024-10-25T08:01:57.807259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_features(features):\n    plt.figure(figsize=(12, 6))\n    plt.plot(features.flatten(), label=\"Feature Intensity\")\n    plt.xlabel(\"Feature Index\")\n    plt.ylabel(\"Intensity\")\n    plt.title(\"Intermediate Forensic Features\")\n    plt.legend()\n    plt.show()\n# Load the data if not already loaded\nX_train, y_train, X_test, y_test = load_data()\n\n# Visualize features of a single character (example with the first test character)\nfeatures = extract_forensic_features(model, X_test[0].reshape(1, 28, 28, 1))\nvisualize_features(features)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:04:19.663464Z","iopub.execute_input":"2024-10-25T08:04:19.664667Z","iopub.status.idle":"2024-10-25T08:04:27.195991Z","shell.execute_reply.started":"2024-10-25T08:04:19.664614Z","shell.execute_reply":"2024-10-25T08:04:27.195054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualize_comparison(X_test, y_test, idx1, idx2, similarity_score):\n    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n    \n    # Show first character\n    axes[0].imshow(X_test[idx1].reshape(28, 28), cmap='gray')\n    axes[0].set_title(f\"Label: {chr(65 + y_test[idx1])}\")\n    \n    # Show second character\n    axes[1].imshow(X_test[idx2].reshape(28, 28), cmap='gray')\n    axes[1].set_title(f\"Label: {chr(65 + y_test[idx2])}\")\n    \n    plt.suptitle(f\"Similarity Score: {similarity_score:.4f}\", fontsize=16)\n    plt.show()\n\n# Example usage\nidx1, idx2 = np.random.randint(0, len(X_test), 2)\nsimilarity_score = compare_characters(model, X_test[idx1], X_test[idx2])\nvisualize_comparison(X_test, y_test, idx1, idx2, similarity_score)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model for sharing\ndef save_model(analyzer, save_path='/kaggle/working/forensic_model'):\n    analyzer.model.save(save_path)\n    print(f\"Model saved to {save_path}\")\n    return save_path\n\n# Example usage\nsave_path = save_model(analyzer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom tensorflow.keras.layers import BatchNormalization, Lambda\nfrom tensorflow.keras.optimizers import Adam\nimport cv2\nfrom scipy.stats import skew, kurtosis\n\nclass ForensicCharacterAnalyzer:\n    def __init__(self):\n        self.model = None\n        \n    def load_data(self):\n        \"\"\"Load and preprocess EMNIST dataset with enhanced normalization\"\"\"\n        print(\"Loading data...\")\n        train_data = pd.read_csv('/kaggle/input/emnist/emnist-letters-train.csv')\n        test_data = pd.read_csv('/kaggle/input/emnist/emnist-letters-test.csv')\n        \n        X_train = train_data.iloc[:, 1:].values.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n        y_train = train_data.iloc[:, 0].values - 1  # Adjust to 0-based indexing\n        \n        X_test = test_data.iloc[:, 1:].values.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n        y_test = test_data.iloc[:, 0].values - 1\n        \n        return X_train, y_train, X_test, y_test\n    \n    from tensorflow.keras.layers import Lambda\n\n    def create_model(self):\n        \"\"\"Create enhanced model with forensic-specific layers\"\"\"\n        input_shape = (28, 28, 1)\n        inputs = Input(shape=input_shape)\n\n        # Stroke Analysis Branch\n        def create_stroke_features(x):\n            # Sobel filters for edge detection\n            sobel_x = tf.image.sobel_edges(x)[..., 0]\n            sobel_y = tf.image.sobel_edges(x)[..., 1]\n\n            # Calculate stroke angles\n            angles = tf.math.atan2(sobel_y, sobel_x)\n            # Calculate stroke intensity (pressure approximation)\n            magnitude = tf.sqrt(tf.square(sobel_x) + tf.square(sobel_y))\n\n            return tf.concat([angles, magnitude], axis=-1)\n\n        stroke_features = Lambda(create_stroke_features)(inputs)\n\n        # Main Feature Extraction Path\n        x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n        x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        # Calculate the output shape of the concatenation manually\n        # Let's assume stroke_features has a shape of (None, 14, 14, 2) after processing\n        output_shape = (14, 14, 34)  # (14, 14, 32) from 'x' and (14, 14, 2) from stroke features\n\n        # Merge stroke features using Lambda layer and specifying output_shape\n        x = Lambda(lambda z: tf.concat([z, stroke_features], axis=-1), output_shape=output_shape)(x)\n\n        # Deeper feature extraction\n        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D((2, 2))(x)\n        x = Dropout(0.25)(x)\n\n        x = Flatten()(x)\n        x = Dense(512, activation='relu', name='forensic_features')(x)\n        x = Dropout(0.5)(x)\n\n        outputs = Dense(26, activation='softmax')(x)\n\n        self.model = Model(inputs=inputs, outputs=outputs)\n        return self.model\n\n\n\n    def analyze_stroke_characteristics(self, image):\n        \"\"\"Analyze stroke characteristics including pressure, angles, and consistency\"\"\"\n        # Convert to numpy array if needed\n        if isinstance(image, tf.Tensor):\n            image = image.numpy()\n        \n        # Ensure proper shape\n        if len(image.shape) == 4:\n            image = image[0]\n        if len(image.shape) == 3:\n            image = image[:,:,0]\n        \n        # Convert to uint8 for OpenCV processing\n        img_uint8 = (image * 255).astype(np.uint8)\n        \n        # Edge detection for stroke analysis\n        edges = cv2.Canny(img_uint8, 50, 150)\n        \n        # Calculate gradients for pressure analysis\n        sobelx = cv2.Sobel(img_uint8, cv2.CV_64F, 1, 0, ksize=3)\n        sobely = cv2.Sobel(img_uint8, cv2.CV_64F, 0, 1, ksize=3)\n        \n        # Stroke angle analysis\n        angles = np.arctan2(sobely, sobelx)\n        magnitudes = np.sqrt(sobelx**2 + sobely**2)\n        \n        # Calculate stroke characteristics\n        stroke_analysis = {\n            'pressure_mean': np.mean(magnitudes),\n            'pressure_std': np.std(magnitudes),\n            'angle_mean': np.mean(angles),\n            'angle_std': np.std(angles),\n            'stroke_consistency': 1.0 / (np.std(magnitudes) + 1e-6),\n            'edge_density': np.sum(edges > 0) / edges.size,\n            'pressure_skew': skew(magnitudes.flatten()),\n            'angle_distribution': np.histogram(angles.flatten(), bins=8)[0].tolist()\n        }\n        \n        return stroke_analysis\n    \n    def extract_forensic_features(self, char_image):\n        \"\"\"Extract deep learning features for forensic analysis\"\"\"\n        feature_model = Model(inputs=self.model.input,\n                            outputs=self.model.get_layer('forensic_features').output)\n        features = feature_model.predict(char_image.reshape(1, 28, 28, 1))\n        \n        # Calculate statistical measures\n        feature_stats = {\n            'mean': np.mean(features),\n            'std': np.std(features),\n            'skewness': skew(features.flatten()),\n            'kurtosis': kurtosis(features.flatten()),\n            'quartiles': np.percentile(features, [25, 50, 75]).tolist(),\n            'feature_vector': features.flatten()\n        }\n        \n        return feature_stats\n    \n    def compare_characters(self, char1, char2):\n        \"\"\"Enhanced character comparison with multiple similarity metrics\"\"\"\n        # Get both feature types\n        feat1_deep = self.extract_forensic_features(char1)\n        feat2_deep = self.extract_forensic_features(char2)\n        \n        stroke1 = self.analyze_stroke_characteristics(char1)\n        stroke2 = self.analyze_stroke_characteristics(char2)\n        \n        # Calculate multiple similarity metrics\n        comparison = {\n            'feature_similarity': self._cosine_similarity(\n                feat1_deep['feature_vector'],\n                feat2_deep['feature_vector']\n            ),\n            'pressure_similarity': 1 - abs(\n                stroke1['pressure_mean'] - stroke2['pressure_mean']\n            ) / max(stroke1['pressure_mean'], stroke2['pressure_mean']),\n            'stroke_consistency_similarity': 1 - abs(\n                stroke1['stroke_consistency'] - stroke2['stroke_consistency']\n            ) / max(stroke1['stroke_consistency'], stroke2['stroke_consistency']),\n            'angle_similarity': self._compare_angle_distributions(\n                stroke1['angle_distribution'],\n                stroke2['angle_distribution']\n            )\n        }\n        \n        # Calculate weighted overall similarity\n        comparison['overall_similarity'] = (\n            0.4 * comparison['feature_similarity'] +\n            0.3 * comparison['pressure_similarity'] +\n            0.2 * comparison['stroke_consistency_similarity'] +\n            0.1 * comparison['angle_similarity']\n        )\n        \n        return comparison\n    \n    def _cosine_similarity(self, v1, v2):\n        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    \n    def _compare_angle_distributions(self, dist1, dist2):\n        \"\"\"Compare angle distributions using Jensen-Shannon divergence\"\"\"\n        dist1 = np.array(dist1) + 1e-10  # Avoid zero division\n        dist2 = np.array(dist2) + 1e-10\n        dist1 = dist1 / np.sum(dist1)\n        dist2 = dist2 / np.sum(dist2)\n        m = 0.5 * (dist1 + dist2)\n        js_div = 0.5 * (np.sum(dist1 * np.log(dist1 / m)) + np.sum(dist2 * np.log(dist2 / m)))\n        return 1 / (1 + js_div)  # Convert to similarity score\n\ndef main():\n    # Initialize analyzer\n    analyzer = ForensicCharacterAnalyzer()\n    \n    # Load data\n    X_train, y_train, X_test, y_test = analyzer.load_data()\n    \n    # Create and compile model\n    model = analyzer.create_model()\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                 loss='sparse_categorical_crossentropy',\n                 metrics=['accuracy'])\n    \n    # Train model\n    print(\"Training model...\")\n    history = model.fit(X_train, y_train,\n                       batch_size=128,\n                       epochs=10,\n                       validation_split=0.1,\n                       verbose=1)\n    \n    # Example analysis\n    print(\"\\nPerforming forensic analysis on sample characters...\")\n    idx1, idx2 = np.random.randint(0, len(X_test), 2)\n    comparison = analyzer.compare_characters(X_test[idx1], X_test[idx2])\n    \n    print(\"\\nForensic Comparison Results:\")\n    for metric, value in comparison.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    return analyzer, history\n\nif __name__ == \"__main__\":\n    analyzer, history = main()","metadata":{"execution":{"iopub.status.busy":"2024-10-25T08:16:46.494319Z","iopub.execute_input":"2024-10-25T08:16:46.494734Z","iopub.status.idle":"2024-10-25T08:16:54.267831Z","shell.execute_reply.started":"2024-10-25T08:16:46.494695Z","shell.execute_reply":"2024-10-25T08:16:54.266151Z"},"trusted":true},"execution_count":null,"outputs":[]}]}